# src/pipeline.py
import os, csv, datetime as dt
import pandas as pd
import numpy as np

from src.universe.load_universe import load_universe
from src.data.fetch_prices import get_prices
from src.news.fetch_news import get_company_news
from src.nlp.sentiment import score_texts
from src.features.aggregate_sentiment import aggregate_daily_sentiment as agg_news
from src.scrapers.fetch_reddit import fetch_reddit_posts
from src.features.aggregate_reddit import tag_tickers, aggregate_daily_reddit as agg_reddit
from src.portfolio.positions import load_positions, save_positions, open_position, close_position

# ---------------- Settings ----------------
UNIVERSE_CSV       = "src/universe/tickers.csv"
LOOKBACK_DAYS      = 180
MIN_PRICE          = 5.0
MIN_AVG_VOL_20D    = 2_000_000

TOP_K_FOR_NEWS     = 15           # fetch news only for top momentum names
TOP_N_BUYS         = 10

# Reddit collection
SUBREDDITS          = ["stocks", "investing", "wallstreetbets"]
REDDIT_LOOKBACK_HRS = 72
REDDIT_LIMIT_PERSUB = 200

# Exit rules
HOLD_DAYS_MAX   = 3               # days
STOP_LOSS       = 0.08            # -8%
TAKE_PROFIT     = 0.05            # +5%
SENT_EXIT_MIN   = 0.20            # blended sentiment below this → SELL

# Weights for final ranking (tweak freely)
W_MOM    = 0.30   # 30% raw momentum (z-scored)
W_RS     = 0.30   # 30% relative strength vs sector (z-scored)
W_NEWS   = 0.25   # 25% news sentiment (raw 0..1)
W_REDDIT = 0.15   # 15% reddit sentiment (raw 0..1)
# ------------------------------------------

# Minimal sector map; extend as you like. Fallback = SPY when not found.
SECTOR_MAP = {
    # Tech
    "AAPL": "XLK", "MSFT": "XLK", "NVDA": "XLK", "AMD": "XLK", "AVGO":"XLK",
    "META": "XLC", "GOOGL":"XLC", "GOOG":"XLC", "NFLX":"XLC", "ADBE":"XLK",
    "CRM":"XLK", "CSCO":"XLK", "ORCL":"XLK", "TXN":"XLK", "QCOM":"XLK",
    # Financials
    "JPM":"XLF", "V":"XLF", "MA":"XLF", "BAC":"XLF", "GS":"XLF", "MS":"XLF", "PYPL":"XLF",
    # Health Care
    "LLY":"XLV", "MRK":"XLV", "TMO":"XLV", "UNH":"XLV", "PFE":"XLV",
    # Consumer
    "AMZN":"XLY", "TSLA":"XLY", "COST":"XLY", "MCD":"XLY", "NKE":"XLY", "WMT":"XLY", "HD":"XLY",
    "PEP":"XLP", "KO":"XLP",
    # Industrials
    "CAT":"XLI", "GE":"XLI",
    # Energy
    "XOM":"XLE", "CVX":"XLE",
    # Materials / Utilities / Real Estate etc. — add as needed
}

os.makedirs("data", exist_ok=True)

def zscore(s: pd.Series) -> pd.Series:
    s = pd.to_numeric(s, errors="coerce")
    mu, sd = s.mean(), s.std(ddof=0)
    return (s - mu) / sd if (sd and sd > 0) else s * 0

def main():
    # 1) Universe + prices/volume
    tickers = load_universe(UNIVERSE_CSV)
    if not tickers:
        raise SystemExit("Universe is empty. Add tickers to src/universe/tickers.csv")

    close, volume = get_prices(tickers, lookback_days=LOOKBACK_DAYS)
    today = close.index.max()
    if pd.isna(today):
        raise SystemExit("No price data returned.")

    ret5 = close.pct_change(5)
    avg_vol20 = volume.rolling(20).mean()

    last_close = close.loc[today].dropna()
    last_avg_vol20 = avg_vol20.loc[today].reindex(last_close.index)

    # 2) Liquidity/price screen
    screened = last_close[(last_close > MIN_PRICE) & (last_avg_vol20 > MIN_AVG_VOL_20D)].index.tolist()
    if len(screened) < 10:
        screened = last_close.index.tolist()  # be forgiving

    # 3) Momentum ranks
    r5_today = ret5.loc[today].reindex(screened).dropna()
    ranked_by_r5 = r5_today.sort_values(ascending=False)

    # 4) NEWS sentiment for top-K momentum names (48h window)
    candidates = ranked_by_r5.head(TOP_K_FOR_NEWS).index.tolist()
    end = dt.datetime.utcnow()
    start = end - dt.timedelta(hours=48)

    news_rows = []
    for t in candidates:
        try:
            arts = get_company_news(t, start, end, page_size=20)
        except Exception:
            arts = []
        titles = [a.get("title") or "" for a in arts]
        scored = score_texts(titles, max_len=128, batch_size=16) if titles else []
        for a, sc in zip(arts, scored):
            news_rows.append({
                "ticker": t,
                "publishedAt": a.get("publishedAt"),
                "title": a.get("title"),
                "sent_label": sc[0],
                "sent_score": float(sc[1]),
            })

    news_df = pd.DataFrame(news_rows)
    if not news_df.empty:
        news_daily = agg_news(news_df)
        news_today = news_daily[news_daily["date"] == dt.datetime.utcnow().date()].set_index("ticker")
    else:
        news_today = pd.DataFrame(columns=["sent_pos","sent_neg","sent_mean","n_news"]).set_index(pd.Index([]))

    # 5) REDDIT posts → tag to tickers → FinBERT (safe truncation) → daily agg
    term_map = {t: [t, f"{t} stock"] for t in screened}  # extend with company names later
    reddit_posts = fetch_reddit_posts(SUBREDDITS, lookback_hours=REDDIT_LOOKBACK_HRS, limit_per_sub=REDDIT_LIMIT_PERSUB)
    reddit_df = pd.DataFrame(reddit_posts)

    def _clip(s, n):
        s = s or ""
        return s[:n]

    if not reddit_df.empty:
        tagged = tag_tickers(reddit_df, term_map)
        texts = [f"{_clip(t,160)}. {_clip(x,540)}"
                 for t, x in zip(tagged["title"].fillna(""), tagged["selftext"].fillna(""))]
        scored = score_texts(texts, max_len=128, batch_size=16) if texts else []
        if scored:
            tagged = tagged.assign(
                sent_label=[s[0] for s in scored],
                sent_score=[float(s[1]) for s in scored]
            )
        reddit_daily = agg_reddit(tagged)
        reddit_today = reddit_daily[reddit_daily["date"] == dt.datetime.utcnow().date()].set_index("ticker")
    else:
        reddit_today = pd.DataFrame(columns=["sent_reddit_mean","reddit_pos","reddit_neg","n_reddit"]).set_index(pd.Index([]))

    # 6) NEW: Relative Strength vs Sector (5d)
    # Fetch benchmark SPY and sector ETFs present in SECTOR_MAP
    sector_etfs = sorted(set(SECTOR_MAP.get(t, "SPY") for t in screened) | {"SPY"})
    sec_close, _ = get_prices(sector_etfs, lookback_days=LOOKBACK_DAYS)
    ret5_sec = sec_close.pct_change(5)

    rs5 = {}
    for t in screened:
        stock_r5 = float(r5_today.get(t, np.nan)) if t in r5_today.index else np.nan
        etf = SECTOR_MAP.get(t, "SPY")
        if etf in ret5_sec.columns and today in ret5_sec.index:
            sec_r5 = float(ret5_sec.loc[today, etf])
        else:
            # Fallback to SPY if ETF not available today
            sec_r5 = float(ret5_sec.loc[today, "SPY"]) if "SPY" in ret5_sec.columns else 0.0
        rs5[t] = stock_r5 - sec_r5  # positive = outperform sector
    rs5_series = pd.Series(rs5).dropna()
    rs5_z = zscore(rs5_series).reindex(r5_today.index).fillna(0.0)

    # 7) Build combined score (momentum z + RS z + sentiments)
    r5_z = zscore(ranked_by_r5)

    s_news = pd.Series(0.0, index=r5_z.index)
    n_news = pd.Series(0, index=r5_z.index, dtype=int)
    if not news_today.empty:
        s_news.update(news_today.get("sent_mean", pd.Series(dtype=float)))
        n_news.update(news_today.get("n_news", pd.Series(dtype=int)))

    s_redd = pd.Series(0.0, index=r5_z.index)
    n_redd = pd.Series(0, index=r5_z.index, dtype=int)
    if not reddit_today.empty:
        s_redd.update(reddit_today.get("sent_reddit_mean", pd.Series(dtype=float)))
        n_redd.update(reddit_today.get("n_reddit", pd.Series(dtype=int)))

    combined = (
        W_MOM    * r5_z.reindex(r5_today.index).fillna(0.0) +
        W_RS     * rs5_z.reindex(r5_today.index).fillna(0.0) +
        W_NEWS   * s_news.reindex(r5_today.index).fillna(0.0) +
        W_REDDIT * s_redd.reindex(r5_today.index).fillna(0.0)
    )

    buy_set = set(combined.sort_values(ascending=False).head(TOP_N_BUYS).index.tolist())

    # 8) Base BUY/HOLD rows for screened names
    rows_base = []
    for t in screened:
        lc = float(last_close.get(t, 0.0))
        r5 = float(r5_today.get(t, 0.0)) if t in r5_today.index else 0.0
        sn = float(s_news.get(t, 0.0))
        sr = float(s_redd.get(t, 0.0))
        nn = int(n_news.get(t, 0))
        nr = int(n_redd.get(t, 0))
        rs = float(rs5.get(t, 0.0))
        action = "BUY" if t in buy_set else "HOLD"
        # confidence: positive momentum + blended sentiment; RS indirectly captured via action
        conf = max(0.0, min(0.99, (max(0, r5) * 10) * 0.35 + (0.65 * (0.6 * sn + 0.4 * sr))))
        reasons = (
            f"r5={r5:.3f}, RS5={rs:.3f}, news={sn:.2f} (n={nn}), "
            f"reddit={sr:.2f} (n={nr}), score={float(combined.get(t,0)):.3f}"
        )
        feats = {
            "ret_5d": round(r5, 4),
            "rel_strength_5d": round(rs, 4),
            "sent_news": round(sn, 2), "n_news": nn,
            "sent_reddit": round(sr, 2), "n_reddit": nr,
            "score": round(float(combined.get(t, 0)), 3)
        }
        rows_base.append([
            dt.date.today().isoformat(), t, action, 0, lc, 0, 0,
            round(conf, 2), reasons, str(feats)
        ])

    # 9) SELL logic for open positions
    positions = load_positions()
    sell_rows = []
    if not positions.empty:
        for _, pos in positions.iterrows():
            t = str(pos["ticker"])
            if t not in last_close.index:
                continue
            entry = float(pos["entry_price"])
            price = float(last_close[t])
            pnl = (price - entry) / entry
            days_held = (dt.date.today() - dt.date.fromisoformat(str(pos["entry_date"]))).days
            r5 = float(r5_today.get(t, 0.0)) if t in r5_today.index else 0.0
            sn = float(s_news.get(t, 0.0))
            sr = float(s_redd.get(t, 0.0))

            triggers, do_sell = [], False
            if pnl <= -STOP_LOSS:            do_sell=True; triggers.append(f"stop {-STOP_LOSS*100:.0f}%")
            if pnl >=  TAKE_PROFIT:          do_sell=True; triggers.append(f"take +{TAKE_PROFIT*100:.0f}%")
            if r5 <= 0:                      do_sell=True; triggers.append("momentum<=0")
            if (0.6 * sn + 0.4 * sr) < SENT_EXIT_MIN:
                                            do_sell=True; triggers.append(f"sent<{SENT_EXIT_MIN:.2f}")
            if days_held >= HOLD_DAYS_MAX:   do_sell=True; triggers.append(f"time>{HOLD_DAYS_MAX}d")

            if do_sell:
                sell_rows.append([
                    dt.date.today().isoformat(), t, "SELL", int(pos.get("qty", 1)), price, 0, 0, 0.80,
                    "; ".join(triggers),
                    str({"pnl": round(float(pnl),4), "r5": round(r5,4), "news": round(sn,2), "reddit": round(sr,2)})
                ])
                positions = close_position(positions, t)

    # 10) Prevent duplicate BUYs for already-open tickers
    open_set = set(positions["ticker"].tolist()) if not positions.empty else set()
    for row in rows_base:
        if row[1] in open_set and row[2] == "BUY":
            row[2] = "HOLD"

    # 11) Write signals (SELL first, then BUY/HOLD)
    header = ["date","ticker","action","qty","entry_price","stop","take_profit","confidence","reasons","features_json"]
    with open("data/signals_latest.csv", "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(header)
        for sr in sell_rows:
            w.writerow(sr)
        for br in rows_base:
            w.writerow(br)

    # 12) Update positions with fresh BUYs (qty=1 paper)
    updated = positions.copy()
    for row in rows_base:
        d, t, action, _, entry_price, *_ = row
        if action == "BUY":
            updated = open_position(updated, t, 1, float(entry_price), d)
    save_positions(updated)

    print("✅ signals_updated.",
          f"Sells: {', '.join([r[1] for r in sell_rows]) or 'None'} |",
          f"Buys: {', '.join(sorted(list(buy_set)))}")

if __name__ == "__main__":
    main()

